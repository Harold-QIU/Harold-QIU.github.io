<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 7.0.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-Apply-LLM-models-to-sepcific-domains" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/12/17/Apply-LLM-models-to-sepcific-domains/" class="article-date">
  <time class="dt-published" datetime="2023-12-17T08:01:35.000Z" itemprop="datePublished">2023-12-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/12/17/Apply-LLM-models-to-sepcific-domains/">Apply LLM models to specific domains</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>This post is about how to apply LLM models to specific domains like clinic, law and finance. Although the LLM models like ChatGPT have showed great performance in open-domain conversation and a strong scalability ability, their hallucination problem is still a big challenge when considering applying them to specific domains. In this post, we will dicuss why build a domian-specific LLM model and how to build it.</p>
<h1 id="Why-build-a-domain-specific-LLM-model"><a href="#Why-build-a-domain-specific-LLM-model" class="headerlink" title="Why build a domain-specific LLM model"></a>Why build a domain-specific LLM model</h1><p>For example, when we use ChatGPT to generate legal advice, it may generate some illegal advice. The reason is that the LLM models are trained on a large-scale dataset, which contains a lot of illegal advice. Therefore, the LLM models will generate illegal advice when they are applied to specific domains. To solve this problem, we need to build a domain-specific LLM model.</p>
<p>Yet, foundational models are far from perfect despite their natural language processing capabilities. Some researchers considered to insert domain knowledge before the prompt to guide the model to generate more reasonable and domain-specific responses. However, the domain knowledge is usually long and most LLMs do not support such long context. Therefore, we need to customize a model with a better language understanding of a specific domain.</p>
<h2 id="Example-of-domain-specific-LLM-model"><a href="#Example-of-domain-specific-LLM-model" class="headerlink" title="Example of domain-specific LLM model"></a>Example of domain-specific LLM model</h2><h3 id="Med-PaLM2"><a href="#Med-PaLM2" class="headerlink" title="Med-PaLM2"></a>Med-PaLM2</h3><p>Med-PaLM 2 is a domain-specific LLM model for medical domain. The model can accurately answer medical questions, putting it on par with medical professionals in some use cases <a target="_blank" rel="noopener" href="https://sites.research.google/med-palm/">^1</a>.</p>
<h3 id="ChatLaw"><a href="#ChatLaw" class="headerlink" title="ChatLaw"></a>ChatLaw</h3><p>ChatLaw is a domain-specific LLM model for legal domain. The model is trained with datasets of Chinese legal domain <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2306.16092">^2</a>.</p>
<h3 id="FinGPT"><a href="#FinGPT" class="headerlink" title="FinGPT"></a>FinGPT</h3><p>FinGPT is a domain-specific LLM model for financial domain <a target="_blank" rel="noopener" href="https://github.com/AI4Finance-Foundation/FinGPT">^3</a>.</p>
<h1 id="How-to-build-a-domain-specific-LLM-model"><a href="#How-to-build-a-domain-specific-LLM-model" class="headerlink" title="How to build a domain-specific LLM model"></a>How to build a domain-specific LLM model</h1><p>There are two ways of building a domain-specific LLM model. One is to fine-tune the pre-trained LLM model on a domain-specific dataset. The other is to train a LLM model from scratch on a domain-specific dataset.</p>
<h2 id="Fine-tune-the-pre-trained-LLM-model"><a href="#Fine-tune-the-pre-trained-LLM-model" class="headerlink" title="Fine-tune the pre-trained LLM model"></a>Fine-tune the pre-trained LLM model</h2><p>The first way is to fine-tune the pre-trained LLM model on a domain-specific dataset. The pre-trained LLM model can be a general LLM model like ChatGPT or a domain-specific LLM model like Med-PaLM2. The domain-specific dataset can be a dataset of a specific domain like medical domain or a dataset of a specific task like question answering task.</p>
<p>Code example:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM, AutoModelForQuestionAnswering</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> Trainer, TrainingArguments</span><br><span class="line"></span><br><span class="line"><span class="comment"># load the pre-trained LLM model</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;microsoft/DialoGPT-medium&quot;</span>)</span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(<span class="string">&quot;microsoft/DialoGPT-medium&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># load the domain-specific dataset</span></span><br><span class="line">dataset = load_dataset(<span class="string">&quot;convai2&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># define the training arguments</span></span><br><span class="line">training_args = TrainingArguments(</span><br><span class="line">    output_dir=<span class="string">&quot;./results&quot;</span>,</span><br><span class="line">    overwrite_output_dir=<span class="literal">True</span>,</span><br><span class="line">    num_train_epochs=<span class="number">3</span>,</span><br><span class="line">    per_device_train_batch_size=<span class="number">4</span>,</span><br><span class="line">    save_steps=<span class="number">1000</span>,</span><br><span class="line">    save_total_limit=<span class="number">3</span>,</span><br><span class="line">    prediction_loss_only=<span class="literal">True</span>,</span><br><span class="line">    logging_steps=<span class="number">1000</span>,</span><br><span class="line">    logging_first_step=<span class="literal">True</span>,</span><br><span class="line">    logging_dir=<span class="string">&quot;./logs&quot;</span>,</span><br><span class="line">    dataloader_num_workers=<span class="number">4</span>,</span><br><span class="line">    seed=<span class="number">42</span>,</span><br><span class="line">    fp16=<span class="literal">True</span>,</span><br><span class="line">    fp16_opt_level=<span class="string">&quot;O2&quot;</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># define the trainer</span></span><br><span class="line">trainer = Trainer(</span><br><span class="line">    model=model,</span><br><span class="line">    args=training_args,</span><br><span class="line">    train_dataset=dataset[<span class="string">&quot;train&quot;</span>],</span><br><span class="line">    eval_dataset=dataset[<span class="string">&quot;validation&quot;</span>],</span><br><span class="line">    tokenizer=tokenizer,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># train the model</span></span><br><span class="line">trainer.train()</span><br></pre></td></tr></table></figure>

<h2 id="Train-a-LLM-model-from-scratch"><a href="#Train-a-LLM-model-from-scratch" class="headerlink" title="Train a LLM model from scratch"></a>Train a LLM model from scratch</h2><p>The second way is to train a LLM model from scratch on a domain-specific dataset. The domain-specific dataset can be a dataset of a specific domain like medical domain or a dataset of a specific task like question answering task. This involves getting the model to learn self-supervised with unlabelled data. During training, the model applies next-token prediction and mask-level modeling. The model attempts to predict words sequentially by masking specific tokens in a sentence.</p>
<p>The code will be long so we will not show it here. You can refer to the code of <a target="_blank" rel="noopener" href="https://github.com/facebookresearch/llama">Llama</a>. And pay attention that some engineering tricks are also needed to train a LLM model from scratch. </p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/12/17/Apply-LLM-models-to-sepcific-domains/" data-id="clqd5g9nk0000nkre8h3q4xby" data-title="Apply LLM models to specific domains" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-hello-world" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/12/17/hello-world/" class="article-date">
  <time class="dt-published" datetime="2023-12-16T16:39:50.066Z" itemprop="datePublished">2023-12-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/12/17/hello-world/">Hello World</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>Welcome to <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a target="_blank" rel="noopener" href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a target="_blank" rel="noopener" href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a target="_blank" rel="noopener" href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/12/17/hello-world/" data-id="clq8afjkx0000o9re0cryahx3" data-title="Hello World" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/12/">December 2023</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2023/12/17/Apply-LLM-models-to-sepcific-domains/">Apply LLM models to specific domains</a>
          </li>
        
          <li>
            <a href="/2023/12/17/hello-world/">Hello World</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2023 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>